{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyM0rdbpnA2cXPr9zqYAsWyj"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VTWapzg3a1vt","executionInfo":{"status":"ok","timestamp":1692335894022,"user_tz":-330,"elapsed":3577,"user":{"displayName":"Shravanthi Murugesan","userId":"15484382523036643515"}},"outputId":"06ae778a-c486-4cda-913a-ec2b6ec44de4"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt"],"metadata":{"id":"sQ8fGkBva7gX","executionInfo":{"status":"ok","timestamp":1692335894667,"user_tz":-330,"elapsed":650,"user":{"displayName":"Shravanthi Murugesan","userId":"15484382523036643515"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["focal_signals = []\n","non_focal_signals = []\n","\n","path = \"/content/drive/MyDrive/BERNBarcelona/\"\n","\n","for i in range(1, 3751):\n","    # Construct the filename\n","    filename = f\"Data_F_Ind{i:04}.txt\"\n","    file_path = path + filename\n","\n","    # Load the data using comma as the delimiter\n","    signal = np.loadtxt(file_path, delimiter=',')\n","    focal_signals.append(signal)\n","\n","for i in range(1, 3751):\n","    # Construct the filename\n","    filename = f\"Data_N_Ind{i:04}.txt\"\n","    file_path = path + filename\n","\n","    # Load the data using comma as the delimiter\n","    signal = np.loadtxt(file_path, delimiter=',')\n","    non_focal_signals.append(signal)"],"metadata":{"id":"cobLUr2PbDD-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Extract the desired row as a pandas Series\n","row = non_focal_signals[0] # Non focal signal\n","\n","# Plot the line chart\n","fig = plt.figure(figsize=(20,4))\n","plt.axhline(0, color='grey')\n","plt.plot(row)\n","plt.xlabel('Time')\n","plt.ylabel('Intensity')\n","plt.title('Non Focal signal')\n","plt.show()"],"metadata":{"id":"3JbWfkMKbFYI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Extract the desired row as a pandas Series\n","row = focal_signals[0] #focal signal\n","\n","# Plot the line chart\n","fig = plt.figure(figsize=(20,4))\n","plt.axhline(0, color='grey')\n","plt.plot(row)\n","plt.xlabel('Time')\n","plt.ylabel('Intensity')\n","plt.title('Focal signal')\n","plt.show()"],"metadata":{"id":"_lBXwzukbI3K"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from scipy.signal import butter, lfilter\n","\n","eeg_signals = non_focal_signals + focal_signals\n","\n","fs_original = 1024\n","fs_downsampled = 512\n","lowcut = 0.5\n","highcut = 150\n","order = 4\n","\n","# Downsample the EEG signals to 512 Hz (if originally sampled at 1024 Hz)\n","# Replace 'fs_original' with the actual original sampling rate of the EEG signals\n","downsample_factor = fs_original // fs_downsampled\n","downsampled_eeg_signals = [signal[::downsample_factor] for signal in eeg_signals]\n","\n","# Apply the filter to the downsampled data using lfilter\n","filtered_eeg_signals = []\n","for signal in downsampled_eeg_signals:\n","    # Compute the filter coefficients for the downsampled signal's sampling rate\n","    nyquist_downsampled = 0.5 * fs_downsampled\n","    low_downsampled = lowcut / nyquist_downsampled\n","    high_downsampled = highcut / nyquist_downsampled\n","    b_downsampled, a_downsampled = butter(order, [low_downsampled, high_downsampled], btype='band')\n","\n","    # Apply the filter using lfilter\n","    filtered_signal = lfilter(b_downsampled, a_downsampled, signal)\n","    filtered_eeg_signals.append(filtered_signal)\n","\n","# Rereference the signals against the median of all the channels\n","median_reference = np.median(filtered_eeg_signals, axis=0)\n","re_referenced_eeg_signals = [signal - median_reference for signal in filtered_eeg_signals]\n"],"metadata":{"id":"GZO9p4o1bLKN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# converting into a numpy array\n","eeg_signals = np.array(re_referenced_eeg_signals)\n","print(eeg_signals.shape)"],"metadata":{"id":"NuRgDSz-bPMM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Create the labels\n","labels = np.zeros((eeg_signals.shape[0],), dtype=np.int32)\n","labels[:3750] = 0 # set the first 200 rows to class 0 -> (non focal signals)\n","labels[3750:] = 1 # set the next 200 rows to class 1 -> (focal signals)\n","print(labels.shape)"],"metadata":{"id":"_-gPA6PWbSon"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Shuffle the data and labels\n","perm = np.random.permutation(len(eeg_signals))\n","data = eeg_signals[perm]\n","labels = labels[perm]\n","\n","# saving the shuffled data and labels as numpy arrays to a file\n","np.save(\"data.npy\", data)\n","np.save(\"labels.npy\",labels)\n","\n","print(data)\n","print(labels)"],"metadata":{"id":"cTVZmbYLbUoP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import numpy as np\n","\n","# Load data\n","data = np.load('data.npy')\n","\n","# Load labels\n","labels = np.load('labels.npy')\n","\n","# Split the data into training and test sets (80:20)\n","train_size = int(0.8 * len(data))\n","train_data, train_labels = data[:train_size], labels[:train_size]\n","test_data, test_labels = data[train_size:], labels[train_size:]\n","\n","# Reshape the data for use with LSTM, GRU, and Conv1D models\n","# train_data = np.reshape(train_data, (train_data.shape[0], train_data.shape[1], 1))\n","# test_data = np.reshape(test_data, (test_data.shape[0], test_data.shape[1], 1))\n","train_data = np.reshape(train_data, (train_data.shape[0], train_data.shape[1], train_data.shape[2]))\n","test_data = np.reshape(test_data, (test_data.shape[0], test_data.shape[1], test_data.shape[2]))"],"metadata":{"id":"HrS_6D25bWmj"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["LSTM with attention"],"metadata":{"id":"unJPw2-0bbQo"}},{"cell_type":"code","source":["import tensorflow\n","from tensorflow import keras\n","from keras.layers import Input, LSTM, Dense, Dropout, Attention\n","from keras.models import Model"],"metadata":{"id":"bjoHK80ibvn7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import tensorflow as tf"],"metadata":{"id":"ij74QX0Lb_1S"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Define the input shape based on your EEG data\n","input_shape = (train_data.shape[1], train_data.shape[2])\n","\n","# Input layer\n","input_layer = Input(shape=input_shape)\n","\n","# LSTM layers\n","lstm_layer1 = LSTM(64, return_sequences=True)(input_layer)\n","lstm_dropout1 = Dropout(0.2)(lstm_layer1)\n","\n","lstm_layer2 = LSTM(64, return_sequences=True)(lstm_dropout1)\n","lstm_dropout2 = Dropout(0.2)(lstm_layer2)\n","\n","# Attention mechanism\n","attention = Attention()([lstm_layer2, lstm_layer2])\n","\n","# Merge attention and LSTM outputs\n","merged = tf.keras.layers.Concatenate()([lstm_layer2, attention])\n","\n","# Fully connected layer\n","dense_layer = Dense(64, activation='relu')(merged)\n","\n","# Output layer\n","output_layer = Dense(1, activation='sigmoid')(dense_layer)\n","\n","\n","# Create the model\n","attention_lstm_model = Model(inputs=input_layer, outputs=output_layer)\n","\n","# Compile the model\n","attention_lstm_model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"],"metadata":{"id":"gd8ri_t9bZI3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_labels = train_labels.reshape((-1, 1))\n","test_labels = test_labels.reshape((-1, 1))\n","#to rectify the value error for the label - shape"],"metadata":{"id":"a9WcX8vej0UY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Train the model\n","attention_history = attention_lstm_model.fit(train_data, train_labels, epochs=10, batch_size=32,\n","                                             validation_data=(test_data, test_labels), verbose=1)"],"metadata":{"id":"a4Qi4xMJcGV7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n","\n","# Evaluate the model\n","loss, accuracy = attention_lstm_model.evaluate(test_data, test_labels)\n","print('Test loss:', loss)\n","print('Test accuracy:', accuracy)\n","\n","# Predict the test labels\n","test_pred = attention_lstm_model.predict(test_data)\n","test_pred = (test_pred > 0.5)\n","\n","# Print the classification report\n","from sklearn.metrics import classification_report\n","print(classification_report(test_labels, test_pred))"],"metadata":{"id":"660vXZ77dc0M"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import seaborn as sns\n","from sklearn.metrics import confusion_matrix\n","\n","# Get the predicted labels\n","test_pred = attention_lstm_model.predict(test_data)\n","test_pred = (test_pred > 0.5)\n","\n","# Get the confusion matrix\n","cm = confusion_matrix(test_labels, test_pred)\n","\n","# Define the class names\n","class_names = ['Negative', 'Positive']\n","\n","# Plot the confusion matrix using seaborn\n","plt.figure(figsize=(5, 4))\n","sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names)\n","plt.title('Confusion Matrix')\n","plt.xlabel('Predicted Label')\n","plt.ylabel('True Label')\n","plt.show()"],"metadata":{"id":"fzI0GvlVDKth"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Get training and validation loss curves\n","train_loss = attention_history.history['loss']\n","val_loss = attention_history.history['val_loss']\n","\n","# Plot loss curves\n","plt.figure(figsize=(5, 4))\n","plt.plot(train_loss, label='Training Loss')\n","plt.plot(val_loss, label='Validation Loss')\n","plt.title('Training and Validation Loss')\n","plt.xlabel('Epochs')\n","plt.ylabel('Loss')\n","plt.legend()\n","plt.show()"],"metadata":{"id":"Q44_gBM0DaOO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Get training and validation accuracy curves\n","train_acc = attention_history.history['accuracy']\n","val_acc = attention_history.history['val_accuracy']\n","\n","# Plot accuracy curves\n","plt.figure(figsize=(5, 4))\n","plt.plot(train_acc, label='Training Accuracy')\n","plt.plot(val_acc, label='Validation Accuracy')\n","plt.title('Training and Validation Accuracy')\n","plt.xlabel('Epochs')\n","plt.ylabel('Accuracy')\n","plt.legend()\n","plt.show()"],"metadata":{"id":"B90HQMdqDhGI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.metrics import roc_curve, auc\n","\n","# Get predicted probabilities for test data\n","y_pred_proba = attention_lstm_model.predict(test_data)\n","\n","# Calculate fpr, tpr, and thresholds\n","fpr, tpr, thresholds = roc_curve(test_labels, y_pred_proba)\n","\n","# Calculate area under curve (AUC)\n","roc_auc = auc(fpr, tpr)\n","\n","# Plot ROC curve\n","plt.figure(figsize=(5, 4))\n","plt.plot(fpr, tpr, label='ROC curve (area = %0.2f)' % roc_auc)\n","plt.plot([0, 1], [0, 1], 'k--')\n","plt.xlim([0.0, 1.0])\n","plt.ylim([0.0, 1.05])\n","plt.xlabel('False Positive Rate')\n","plt.ylabel('True Positive Rate')\n","plt.title('Receiver Operating Characteristic')\n","plt.legend(loc=\"lower right\")\n","plt.show()"],"metadata":{"id":"LnRnRvLPDljX"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Transformer"],"metadata":{"id":"6V_p9be6dGuY"}},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","from torch.nn import Transformer"],"metadata":{"id":"2GbyVDkLcMS9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class TransformerModel(nn.Module):\n","    def __init__(self, input_size, d_model, nhead, num_layers, num_classes):\n","        super(TransformerModel, self).__init__()\n","        self.embedding = nn.Embedding(input_size, d_model)\n","        self.transformer = nn.Transformer(d_model, nhead, num_layers)\n","        self.fc = nn.Linear(d_model, num_classes)\n","\n","    def forward(self, x):\n","        x = self.embedding(x)\n","        x = self.transformer(x)\n","        x = self.fc(x)\n","        return x"],"metadata":{"id":"Ci9Wdjf0d7YG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["input_size = 10000\n","d_model = 128\n","nhead = 8\n","num_layers = 4\n","num_classes = 2"],"metadata":{"id":"c6o9W_rFd_Ev"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Hyerparameters\n","#d_model\n","#input size- actual size of vocabulary - not sure\n","#nhead- number of attention heads\n","#num_layers- number of stacked transformer layers\n","#num_classes- number of output classes"],"metadata":{"id":"ZcVNX1uEeFAc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model = TransformerModel(input_size, d_model, nhead, num_layers, num_classes)"],"metadata":{"id":"I4HPCRAeeB2p"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"usor7HpykmJr"},"execution_count":null,"outputs":[]}]}